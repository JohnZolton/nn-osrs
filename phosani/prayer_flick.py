"""
Simple tinygrad script to teach a small MLP to "prayer-flick" on a 4-tick cycle.

Problem setup (toy / symbolic):
- Boss attacks every 4 ticks (timer values 4 -> 3 -> 2 -> 1 -> 4 ...).
- At the tick when timer == 1 the boss performs an attack of type {magic, ranged, melee}.
- Desired behavior (labels):
    * If timer != 1 -> action = 0 (no prayer)
    * If timer == 1 -> action = 1 (protect_magic) / 2 (protect_ranged) / 3 (protect_melee)
- Agent observes a compact state per tick:
    * one-hot encoding of boss_attack_timer (4 dims)
    * previous action one-hot (4 dims) -- optional context
    => input dim = 8
- Model: small MLP (8 -> 32 -> 16 -> 4) trained with cross-entropy.
- Training: supervised learning using synthetic trajectories generated by the environment rules.

This is a minimal, deterministic toy task so a small network will learn it quickly.
You can later switch to an RL setup if you want the agent to discover flicking from sparse rewards.
"""

import random
import numpy as np
from tinygrad.tensor import Tensor
import tinygrad.nn.optim as optim

# reproducibility
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
Tensor.manual_seed(SEED)

# constants
TIMER_CYCLE = 4
NUM_ACTIONS = 4  # 0: none, 1: magic, 2: ranged, 3: melee

def one_hot_np(indices, num_classes):
    b = indices.shape[0]
    oh = np.zeros((b, num_classes), dtype=np.float32)
    oh[np.arange(b), indices] = 1.0
    return oh

def generate_dataset(num_sequences=2000, seq_len=40):
    """
    Generate synthetic sequences. For each sequence:
      - choose random attack types (for each attack event)
      - produce per-tick observations (timer one-hot + prev_action one-hot)
      - produce per-tick labels (0 for no prayer, or correct protection on timer==1)
    Returns flattened dataset: X shape (num_sequences*seq_len, input_dim), y shape (N,)
    """
    inputs = []
    labels = []

    for _ in range(num_sequences):
        # Pre-generate attack types for each attack event (one per 4-tick cycle)
        num_attacks = (seq_len + TIMER_CYCLE - 1) // TIMER_CYCLE + 1
        attack_types = np.random.randint(0, 3, size=(num_attacks,))  # 0=magic,1=ranged,2=melee

        prev_action = 0  # start with no prayer
        attack_idx = 0
        timer = TIMER_CYCLE

        for t in range(seq_len):
            # observation: timer one-hot (4) + prev_action one-hot (4) => 8-dim
            timer_oh = one_hot_np(np.array([timer - 1]), TIMER_CYCLE).reshape(-1)  # timer 1..4 -> index 0..3
            prev_oh = one_hot_np(np.array([prev_action]), NUM_ACTIONS).reshape(-1)
            obs = np.concatenate([timer_oh, prev_oh]).astype(np.float32)

            # label: if timer == 1 -> protect for attack_types[attack_idx], else 0
            if timer == 1:
                true_attack = attack_types[attack_idx]  # 0..2
                label = true_attack + 1  # map 0->1(magic),1->2(ranged),2->3(melee)
                attack_idx += 1
            else:
                label = 0

            inputs.append(obs)
            labels.append(label)

            # simulate that the agent acted according to label (for generating next prev_action)
            prev_action = label

            # step timer
            timer -= 1
            if timer == 0:
                timer = TIMER_CYCLE

    X = np.stack(inputs, axis=0)
    y = np.array(labels, dtype=np.int32)
    return X, y

# tinygrad model
class FlickNet:
    def __init__(self, input_dim):
        # simple MLP parameters
        self.w1 = Tensor.randn(input_dim, 32) * 0.1
        self.b1 = Tensor.zeros(32)
        self.w2 = Tensor.randn(32, 16) * 0.1
        self.b2 = Tensor.zeros(16)
        self.w3 = Tensor.randn(16, NUM_ACTIONS) * 0.1
        self.b3 = Tensor.zeros(NUM_ACTIONS)

        # ensure grads
        self.params = [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]
        for p in self.params:
            p.requires_grad_(True)

    def __call__(self, x: Tensor) -> Tensor:
        # x: (B, input_dim)
        h = x.dot(self.w1) + self.b1
        h = h.relu()
        h = h.dot(self.w2) + self.b2
        h = h.relu()
        logits = h.dot(self.w3) + self.b3
        return logits

    def parameters(self):
        return self.params

def cross_entropy_logits(logits: Tensor, targets: np.ndarray):
    """
    logits: Tensor (B, C)
    targets: np.ndarray int (B,) with class indices
    returns: scalar Tensor loss (mean)
    """
    # convert targets to one-hot in Tensor
    targets_t = Tensor.one_hot(Tensor(targets.astype(np.int32)), NUM_ACTIONS) if hasattr(Tensor, "one_hot") else Tensor(one_hot_np(targets, NUM_ACTIONS))
    # tinygrad has .cross_entropy in newer versions; implement directly:
    log_probs = logits.log_softmax(axis=1)
    # negative log-likelihood
    nll = -(log_probs * targets_t).sum(axis=1)
    return nll.mean()

def accuracy(logits_t: Tensor, targets_np: np.ndarray):
    preds = np.argmax(logits_t.numpy(), axis=1)
    return (preds == targets_np).mean()

if __name__ == "__main__":
    # generate dataset
    X, y = generate_dataset(num_sequences=2000, seq_len=40)  # 80k samples
    # shuffle / split
    idx = np.random.permutation(len(X))
    train_idx = idx[: int(0.9 * len(X))]
    val_idx = idx[int(0.9 * len(X)):]

    X_train, y_train = X[train_idx], y[train_idx]
    X_val, y_val = X[val_idx], y[val_idx]

    input_dim = X.shape[1]
    model = FlickNet(input_dim)
    opt = optim.Adam(model.parameters(), lr=1e-3)
    # enable training mode for tinygrad (required by optimizer)
    Tensor.training = True

    batch_size = 256
    epochs = 20

    for ep in range(epochs):
        # shuffle
        perm = np.random.permutation(len(X_train))
        losses = []
        for i in range(0, len(perm), batch_size):
            batch_idx = perm[i: i + batch_size]
            xb = Tensor(X_train[batch_idx])
            yb = y_train[batch_idx]

            logits = model(xb)
            # compute loss
            # tinygrad may have sparse cross-entropy helper, but compute manually
            # convert targets to Tensor one-hot manually
            # create target one-hot numpy then Tensor
            t_oh = one_hot_np(yb, NUM_ACTIONS)
            t_oh_t = Tensor(t_oh)
            logp = logits.log_softmax(axis=1)
            nll = -(logp * t_oh_t).sum(axis=1).mean()

            opt.zero_grad()
            nll.backward()
            opt.step()
            losses.append(nll.item())

        # evaluation
        with_val_logits = model(Tensor(X_val))
        val_acc = accuracy(with_val_logits, y_val)
        print(f"Epoch {ep+1}/{epochs} | TrainLoss {np.mean(losses):.4f} | ValAcc {val_acc*100:.2f}%")

    # quick interactive test
    print("Testing a short simulated sequence (timer countdown):")
    env_timer = TIMER_CYCLE
    prev_action = 0
    for t in range(12):
        obs = np.concatenate([one_hot_np(np.array([env_timer - 1]), TIMER_CYCLE).reshape(-1),
                              one_hot_np(np.array([prev_action]), NUM_ACTIONS).reshape(-1)]).astype(np.float32)
        logits = model(Tensor(obs))
        pred = int(np.argmax(logits.numpy()))
        print(f"Tick {t:02d} Timer {env_timer} -> PredAction {pred}")
        # simulate true label for printing
        true_label = 0
        if env_timer == 1:
            true_label = np.random.randint(1, 4)  # random attack type, not identical to training generator here
        print(f"  true_label (simulated): {true_label}")
        prev_action = pred
        env_timer -= 1
        if env_timer == 0:
            env_timer = TIMER_CYCLE

    print("Done.")
